{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Importaciones\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import nest_asyncio\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from llama_index.core import SimpleDirectoryReader, Document, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('MISTRAL_API_KEY')\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de modelos y directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Configuración de modelos y directorios\n",
    "# Configurar modelos por defecto\n",
    "Settings.llm = ChatMistralAI(model=\"mistral-small-latest\")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Crear directorio para embeddings si no existe\n",
    "embeddings_dir = Path(\"data/embeddings\")\n",
    "embeddings_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de gestión de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Funciones de gestión de embeddings\n",
    "def save_embeddings(nodes, doc_name):\n",
    "    \"\"\"Guarda los embeddings en un archivo.\"\"\"\n",
    "    embeddings_path = embeddings_dir / f\"{doc_name}_embeddings.pkl\"\n",
    "    with open(embeddings_path, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "\n",
    "def load_embeddings(doc_name):\n",
    "    \"\"\"Carga los embeddings desde un archivo.\"\"\"\n",
    "    embeddings_path = embeddings_dir / f\"{doc_name}_embeddings.pkl\"\n",
    "    if embeddings_path.exists():\n",
    "        with open(embeddings_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función principal para crear el motor de consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Función principal para crear el motor de consultas\n",
    "def get_router_query_engine(file_path, force_reload=False):\n",
    "    \"\"\"\n",
    "    Crea un motor de consultas con soporte para caché de embeddings.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Ruta al archivo PDF\n",
    "        force_reload (bool): Si es True, recrea los embeddings aunque existan en caché\n",
    "    \n",
    "    Returns:\n",
    "        RouterQueryEngine: Motor de consultas configurado\n",
    "    \"\"\"\n",
    "    # Obtener nombre del documento\n",
    "    doc_name = Path(file_path).stem\n",
    "    \n",
    "    # Intentar cargar embeddings desde caché\n",
    "    nodes = None\n",
    "    if not force_reload:\n",
    "        nodes = load_embeddings(doc_name)\n",
    "    \n",
    "    # Si no hay embeddings en caché o se fuerza la recarga\n",
    "    if nodes is None:\n",
    "        # Cargar documentos\n",
    "        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "        \n",
    "        # Crear nodos\n",
    "        splitter = SentenceSplitter(chunk_size=1024)\n",
    "        nodes = splitter.get_nodes_from_documents(documents)\n",
    "        \n",
    "        # Guardar embeddings para uso futuro\n",
    "        save_embeddings(nodes, doc_name)\n",
    "    \n",
    "    # Crear índices\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    vector_index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)\n",
    "    \n",
    "    # Configurar motores de consulta\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "        llm=Settings.llm\n",
    "    )\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)\n",
    "    \n",
    "    # Crear herramientas de consulta\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(\n",
    "            \"Útil para preguntas de resumen relacionadas con el documento\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=(\n",
    "            \"Útil para recuperar contexto específico del documento\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Crear motor de consultas con enrutamiento\n",
    "    query_engine = RouterQueryEngine(\n",
    "        selector=LLMSingleSelector.from_defaults(),\n",
    "        query_engine_tools=[\n",
    "            summary_tool,\n",
    "            vector_tool,\n",
    "        ],\n",
    "        verbose=True\n",
    "    )\n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for the main points of the document, which aligns with summarizing the document..\n",
      "\u001b[0mThe main points of the document are:\n",
      "\n",
      "- Introduction of a framework called MetaGPT, designed for multi-agent collaboration based on large language models (LLMs).\n",
      "- MetaGPT incorporates Standardized Operating Procedures (SOPs) to streamline workflows, reduce errors, and maintain consistency in communication.\n",
      "- The framework utilizes an assembly line paradigm, assigning diverse roles to various agents to break down complex tasks into subtasks.\n",
      "- MetaGPT achieves state-of-the-art performance in collaborative software engineering benchmarks, generating more coherent solutions than previous chat-based multi-agent systems.\n",
      "- The system uses specialized roles like Product Manager, Architect, Project Manager, Engineer, and QA Engineer to handle complex software development tasks.\n",
      "- MetaGPT emphasizes structured communication and the use of a publish-subscribe mechanism and shared message pool to facilitate information exchange and reduce communication overhead.\n",
      "- The framework includes an iterative programming process with executable feedback to improve code quality and ensure runtime correctness.\n",
      "- MetaGPT demonstrates superior performance in code generation, executability, and productivity compared to other methods, validated through benchmarks like HumanEval, MBPP, and a custom SoftwareDev dataset.\n",
      "- The system's capabilities are highlighted through various metrics, including executability, cost, code statistics, productivity, and human revision cost.\n",
      "- MetaGPT is designed to handle complex software engineering tasks efficiently, incorporating SOPs and structured communication to enhance code generation.\n",
      "- The document discusses the challenges faced by existing LLM-based multi-agent systems, such as logic inconsistencies and cascading hallucinations, and how MetaGPT addresses these issues.\n",
      "- The framework includes a communication protocol that enhances role communication efficiency, structured communication interfaces, and an effective publish-subscribe mechanism.\n",
      "- MetaGPT introduces an executive feedback mechanism that debugs and executes code during runtime, significantly elevating code generation quality.\n",
      "- The document highlights the contributions of MetaGPT, including its innovative integration of human-like SOPs and its ability to handle higher levels of software complexity.\n",
      "- MetaGPT is validated using publicly available benchmarks and demonstrates a 100% task completion rate in experimental evaluations.\n",
      "- The document compares MetaGPT with other frameworks like AutoGPT, LangChain, AgentVerse, and ChatDev, highlighting its superior performance in generating executable code.\n",
      "- The system supports dynamic teamwork and self-organization, similar to a market economy where agents are rewarded based on their contributions.\n",
      "- The document outlines a detailed software development process using MetaGPT, from user input to the delivery of a functional application.\n",
      "- The document mentions ethical concerns related to the use of MetaGPT, such as potential unemployment, transparency, accountability, privacy, and data security.\n",
      "- The document discusses various software development tasks and projects, including game development, data processing, music and news applications, and weather-related programs.\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Ejemplo de uso\n",
    "# Inicializar motor de consultas (primera vez creará y guardará los embeddings)\n",
    "query_engine = get_router_query_engine(\"../documents/metagpt.pdf\")\n",
    "\n",
    "# Ejemplo de consulta\n",
    "response = query_engine.query(\"¿Cuáles son los puntos principales del documento?\")\n",
    "print(str(response))\n",
    "\n",
    "# Si necesitas forzar la recarga de embeddings (por ejemplo, si el documento ha cambiado)\n",
    "# query_engine = get_router_query_engine(\"../documents/metagpt.pdf\", force_reload=True)\n",
    "# response = query_engine.query(\"¿Cuáles son los puntos principales del documento?\")\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for specific lines from the document, which aligns with recovering specific context from the document..\n",
      "\u001b[0mThe document begins with a table that presents additional results of a system called MetaGPT without feedback on software development tasks. The table includes various statistics for 10 randomly selected tasks, such as the number of code files, lines of code, lines per code file, number of documentation files, lines of documentation, lines per documentation file, prompt tokens, completion tokens, time costs, and money costs. Additionally, it lists the cost of revision, code executability, and specific issues encountered in each task.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Dame las 10 primeras lineas del documento\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar el llm de mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm = MistralAI(model=\"mistral-small-latest\")\n",
    "\n",
    "# Define a prompt that guides Mistral to use tools\n",
    "prompt_template = PromptTemplate(\n",
    "    \"\"\"Given a user query and a set of available tools, determine which tool is most appropriate to use to answer the query.\n",
    "\n",
    "You have access to the following tools:\n",
    "{{tool_desc}}\n",
    "\n",
    "Carefully consider the user's request and the description of each tool to decide which one can best address the query.\n",
    "\n",
    "Once you have selected a tool, you must respond with a JSON object in the following format:\n",
    "{{{{\n",
    "  \"tool_code\": \"<name_of_the_selected_tool>\",\n",
    "  \"tool_args\": {{\"<arg_name>\": <value>, \"<arg_name>\": <value>, ...}}\n",
    "}}}}\n",
    "\n",
    "If the user's query can be answered directly without using any of the provided tools, respond with a JSON object indicating that no tool is needed:\n",
    "{{{{\n",
    "  \"tool_code\": \"none\",\n",
    "  \"tool_args\": {{}}\n",
    "}}}}\n",
    "\n",
    "User Query: {{query_str}}\n",
    "\"\"\"\n",
    ")\n",
    "# Create a custom output parser for ReAct-like behavior\n",
    "class SimpleToolOutputParser(ReActOutputParser):\n",
    "    def format_tools_as_string(self, tools):\n",
    "        return \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
    "\n",
    "    def parse_tool_code(self, output: str) -> str:\n",
    "        # Simple extraction assuming \"tool_code\": \"tool_name\" format\n",
    "        import json\n",
    "        try:\n",
    "            return json.loads(output)[\"tool_code\"]\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return None\n",
    "\n",
    "    def parse_tool_args(self, output: str) -> dict:\n",
    "        # Simple extraction assuming \"tool_args\": {\"arg1\": value, ...} format\n",
    "        import json\n",
    "        try:\n",
    "            return json.loads(output)[\"tool_args\"]\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return {}\n",
    "\n",
    "# Instantiate the custom output parser\n",
    "output_parser = SimpleToolOutputParser()\n",
    "\n",
    "# Create a custom chat formatter (optional, but can help guide Mistral)\n",
    "chat_formatter = ReActChatFormatter(\n",
    "    system_prompt_template=prompt_template,\n",
    "    prompt_type=PromptType.SIMPLE_INPUT,\n",
    ")\n",
    "\n",
    "# Format the prompt for Mistral\n",
    "formatted_messages = chat_formatter.format(\n",
    "    tools=[add_tool, mystery_tool],\n",
    "    chat_history=[ChatMessage(role=MessageRole.USER, content=\"What is mystery(2, 9)?\")],\n",
    "    current_reasoning=None\n",
    ")\n",
    "# Predict using Mistral\n",
    "user_query_content = formatted_messages[-1].content\n",
    "final_prompt = PromptTemplate(\"{query_str}\")\n",
    "response_str = llm.predict(final_prompt, query_str=user_query_content)\n",
    "print(f\"Mistral's raw response:\\n{response_str}\\n\")\n",
    "\n",
    "# Parse the output to get the tool and arguments\n",
    "tool_code = output_parser.parse_tool_code(response_str)\n",
    "tool_args = output_parser.parse_tool_args(response_str)\n",
    "\n",
    "print(f\"Selected tool: {tool_code}\")\n",
    "print(f\"Tool arguments: {tool_args}\")\n",
    "\n",
    "# Execute the tool if one was selected\n",
    "if tool_code == \"mystery_function\":\n",
    "    result = mystery(tool_args.get(\"x\"), tool_args.get(\"y\"))\n",
    "    print(f\"\\nOutput of mystery_function(2, 9): {result}\")\n",
    "elif tool_code == \"add\":\n",
    "    result = add(tool_args.get(\"a\"), tool_args.get(\"b\"))\n",
    "    print(f\"\\nOutput of add(): {result}\")\n",
    "else:\n",
    "    print(\"\\nNo tool was selected, or the tool code was not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query_engine = get_router_query_engine(\"../documents/metagpt.pdf\")\n",
    "response = query_engine.query(\"Dime las competencias clave que salen en el documento\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-multidoc-agent-0etcU5GM-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
